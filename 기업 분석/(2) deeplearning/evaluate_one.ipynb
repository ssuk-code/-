{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "# 첫 번째 차원이 784인 2D 텐서만 입력으로 받는 층, 첫 번째 차원의 크기가 32로 변환된 텐서를 출력.\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "# 케라스에서는 모델에 추가된 층을 자동으로 상위 층의 크기에 맞추어 줌.\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'rt', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        # txt 파일의 헤더(id document label)는 제외하기\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "train_data = read_data('./ratings_train.txt')\n",
    "test_data = read_data('./ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['아/Exclamation',\n",
      "  '더빙/Noun',\n",
      "  '../Punctuation',\n",
      "  '진짜/Noun',\n",
      "  '짜증나다/Adjective',\n",
      "  '목소리/Noun'],\n",
      " '0')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm은 정규화, stem은 근어로 표시하기를 나타냄\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "if os.path.isfile('train_docs.json'):\n",
    "    with open('train_docs.json', 'a', encoding='utf-8') as f:\n",
    "        train_docs = json.load(f)\n",
    "    with open('test_docs.json', 'a', encoding='utf-8') as f:\n",
    "        test_docs = json.load(f)\n",
    "else:\n",
    "    train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
    "    test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
    "    # JSON 파일로 저장\n",
    "    with open('train_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(train_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "    with open('test_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(test_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "\n",
    "# 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용\n",
    "pprint(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159921\n"
     ]
    }
   ],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens, name='NMSC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간이 꽤 걸립니다! 시간을 절약하고 싶으면 most_common의 매개변수를 줄여보세요.\n",
    "selected_words = [f[0] for f in text.vocab().most_common(1000)]\n",
    "\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "train_x = [term_frequency(d) for d, _ in train_docs]\n",
    "test_x = [term_frequency(d) for d, _ in test_docs]\n",
    "train_y = [c for _, c in train_docs]\n",
    "test_y = [c for _, c in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.asarray(train_x).astype('float32')\n",
    "x_test = np.asarray(test_x).astype('float32')\n",
    "\n",
    "y_train = np.asarray(train_y).astype('float32')\n",
    "y_test = np.asarray(test_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "150000/150000 [==============================] - 4s 29us/sample - loss: 0.4320 - binary_accuracy: 0.80221s - loss: 0.4488 - bin\n",
      "Epoch 2/10\n",
      "150000/150000 [==============================] - 4s 26us/sample - loss: 0.3810 - binary_accuracy: 0.8247\n",
      "Epoch 3/10\n",
      "150000/150000 [==============================] - 4s 24us/sample - loss: 0.3654 - binary_accuracy: 0.8331\n",
      "Epoch 4/10\n",
      "150000/150000 [==============================] - 3s 22us/sample - loss: 0.3519 - binary_accuracy: 0.84182s - los\n",
      "Epoch 5/10\n",
      "150000/150000 [==============================] - 4s 24us/sample - loss: 0.3401 - binary_accuracy: 0.8480\n",
      "Epoch 6/10\n",
      "150000/150000 [==============================] - 4s 24us/sample - loss: 0.3281 - binary_accuracy: 0.8553\n",
      "Epoch 7/10\n",
      "150000/150000 [==============================] - 4s 24us/sample - loss: 0.3166 - binary_accuracy: 0.8608\n",
      "Epoch 8/10\n",
      "150000/150000 [==============================] - 4s 25us/sample - loss: 0.3053 - binary_accuracy: 0.8673\n",
      "Epoch 9/10\n",
      "150000/150000 [==============================] - 4s 28us/sample - loss: 0.2937 - binary_accuracy: 0.8733\n",
      "Epoch 10/10\n",
      "150000/150000 [==============================] - 4s 29us/sample - loss: 0.2827 - binary_accuracy: 0.8784\n",
      "50000/50000 [==============================] - 3s 62us/sample - loss: 0.3972 - binary_accuracy: 0.8262\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(1000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss=losses.binary_crossentropy,\n",
    "             metrics=[metrics.binary_accuracy])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['전윙크하고있는데나름만족해요 윙크는추천인경로로들어가면커피쿠폰받고1주일체험가능하니필요하시면챗이나쪽지주세요',\n",
       " '  윙크 테스트 일주정도해보세요 저희아기 5세신청해서그런지 아님 제가 옆에 지키지않아서인지 안맞더라구요 그래서 눈높이 현재하고있어요 처음 방문수업하고 현재는 화수목 학원에서 하고있어요 딸이 잘적응하고 빨리배우는거 같아요 좀 힘들긴하지만 나름 행복해요',\n",
       " '  선생님이 제일 중요한것같아요씽크빅중인데 경험이많으셔서 잘가르쳐주시고 패드도잘쓰고있어요체험해보시고 아이랑맞는학습지 시키세요',\n",
       " '  웅진은 얼만가요?',\n",
       " '  웅진도 종류가 많던데 누리패키지세과목 선생님방문없이 5만9천원 2년약정이더라구요']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "feedback_list=[]\n",
    "\n",
    "with open('../(1) cleaner/wj_thinkbig_feedback.csv', 'r', encoding='utf-8') as f:\n",
    "    csv_reader = csv.reader(f, delimiter = '\\n')\n",
    "    for row in csv_reader:\n",
    "        feedback_list.append(row)\n",
    "\n",
    "feedback_list = sum(feedback_list, [])\n",
    "feedback_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feedback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos_neg_fix(review):\n",
    "    token = tokenize(review)\n",
    "    tf = term_frequency(token)\n",
    "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data))\n",
    "    if(score > 0.5):\n",
    "        positive001.append(\"[{}], 긍정 텍스트 확률 : {:.2f}%\".format(review, score * 100))\n",
    "    else:\n",
    "        negative001.append(\"[{}], 부정 텍스트 확률 : {:.2f}%\".format(review, (1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive001 = []\n",
    "negative001 = []\n",
    "\n",
    "for i in range(0, len(feedback_list)):\n",
    "    predict_pos_neg_fix(str(feedback_list[i]))\n",
    "\n",
    "import pandas as pd\n",
    "evaluate_pos001 = pd.DataFrame(positive001)\n",
    "evaluate_pos001.to_csv(\"./wj_thinkbig_feedback_positive.csv\", header = False, index = False)\n",
    "evaluate_neg001 = pd.DataFrame(negative001)\n",
    "evaluate_neg001.to_csv(\"./wj_thinkbig_feedback_negative.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한글 시작하려고하는데요웅진씽크빅 누리패키지한글.수학.영어윙크한글.수학.영어둘중머가나을까요? 해보신분잇나요..며칠째 계속 검색중인데고민되네요..',\n",
       " ' 그렇게 육아교육전같은거 가면 신규회원 모집하려고 하던데한글깨치기랑 수학깨치기하다가창의아트깨치기 추가하려니까 선생님이 처음엔과목하는데 한과목 추가하면 애가 지루하지않겠냐 하면서 시작하시던데나중엔 수업이 빡빡하다면서월에 선생님.바뀌니까 그때 스케줄조절해서 하라고 이실직고하네요근데 저는 지금부터 해주고싶다고했더니 안된다네요무조건안된다고 기다리래요아 뭔가 웅진씽크빅...영업은 실컷해서 고객유치는 하면서 관리는 안되는것같네요한과목 더하고싶어도 못하니까넘 기분이 아리송ㅎㅎ 찜찜하네요다른 학습지도 그런가요? 년만 해보고 학습지 바꿔볼까도 싶구요ㅎㅎ',\n",
       " ' 안녕하세요세 남아 지금 구몬 방문학습중이구요웅진씽크빅으로 옮겨볼까하는데..웅진 방문수업 하시는 맘님들 어떠신가요?혹 저처럼 구몬에서 웅진으로갈아타신분들 계시면 후기좀 부탁드려요너무 고민중이라 소중한댓글 기다립니다여기는 배방 장재리구요장재리쪽 선생님은 어떠신지도 궁금하네요',\n",
       " ' 현재 세',\n",
       " '세 인데요한솔을 하고 있는데 너무 제자리 인거 같고 질릴거 같아서 다른걸로갈아 타볼까 하는데 웅진 씽크빅 한글깨치기나 그 외 다른과목들 어떤가요??']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "feedback_list=[]\n",
    "\n",
    "with open('../(1) cleaner/wj_thinkbig_content.csv', 'r', encoding='utf-8') as f:\n",
    "    csv_reader = csv.reader(f, delimiter = '\\n')\n",
    "    for row in csv_reader:\n",
    "        feedback_list.append(row)\n",
    "\n",
    "feedback_list = sum(feedback_list, [])\n",
    "feedback_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feedback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos_neg_fix(review):\n",
    "    token = tokenize(review)\n",
    "    tf = term_frequency(token)\n",
    "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data))\n",
    "    if(score > 0.5):\n",
    "        positive001.append(\"[{}], 긍정 텍스트 확률 : {:.2f}%\".format(review, score * 100))\n",
    "    else:\n",
    "        negative001.append(\"[{}], 부정 텍스트 확률 : {:.2f}%\".format(review, (1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive001 = []\n",
    "negative001 = []\n",
    "\n",
    "for i in range(0, len(feedback_list)):\n",
    "    predict_pos_neg_fix(str(feedback_list[i]))\n",
    "\n",
    "import pandas as pd\n",
    "evaluate_pos001 = pd.DataFrame(positive001)\n",
    "evaluate_pos001.to_csv(\"./wj_thinkbig_content_positive.csv\", header = False, index = False)\n",
    "evaluate_neg001 = pd.DataFrame(negative001)\n",
    "evaluate_neg001.to_csv(\"./wj_thinkbig_content_negative.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
