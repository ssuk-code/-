{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "# 첫 번째 차원이 784인 2D 텐서만 입력으로 받는 층, 첫 번째 차원의 크기가 32로 변환된 텐서를 출력.\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "# 케라스에서는 모델에 추가된 층을 자동으로 상위 층의 크기에 맞추어 줌.\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'rt', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        # txt 파일의 헤더(id document label)는 제외하기\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "train_data = read_data('./ratings_train.txt')\n",
    "test_data = read_data('./ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['아/Exclamation',\n",
      "  '더빙/Noun',\n",
      "  '../Punctuation',\n",
      "  '진짜/Noun',\n",
      "  '짜증나다/Adjective',\n",
      "  '목소리/Noun'],\n",
      " '0')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm은 정규화, stem은 근어로 표시하기를 나타냄\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "if os.path.isfile('train_docs.json'):\n",
    "    with open('train_docs.json', 'a', encoding='utf-8') as f:\n",
    "        train_docs = json.load(f)\n",
    "    with open('test_docs.json', 'a', encoding='utf-8') as f:\n",
    "        test_docs = json.load(f)\n",
    "else:\n",
    "    train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
    "    test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
    "    # JSON 파일로 저장\n",
    "    with open('train_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(train_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "    with open('test_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(test_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "\n",
    "# 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용\n",
    "pprint(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159921\n"
     ]
    }
   ],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens, name='NMSC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간이 꽤 걸립니다! 시간을 절약하고 싶으면 most_common의 매개변수를 줄여보세요.\n",
    "selected_words = [f[0] for f in text.vocab().most_common(1000)]\n",
    "\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "train_x = [term_frequency(d) for d, _ in train_docs]\n",
    "test_x = [term_frequency(d) for d, _ in test_docs]\n",
    "train_y = [c for _, c in train_docs]\n",
    "test_y = [c for _, c in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.asarray(train_x).astype('float32')\n",
    "x_test = np.asarray(test_x).astype('float32')\n",
    "\n",
    "y_train = np.asarray(train_y).astype('float32')\n",
    "y_test = np.asarray(test_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "150000/150000 [==============================] - 5s 32us/sample - loss: 0.4314 - binary_accuracy: 0.8030\n",
      "Epoch 2/10\n",
      "150000/150000 [==============================] - 3s 21us/sample - loss: 0.3815 - binary_accuracy: 0.8247\n",
      "Epoch 3/10\n",
      "150000/150000 [==============================] - 3s 20us/sample - loss: 0.3653 - binary_accuracy: 0.8333\n",
      "Epoch 4/10\n",
      "150000/150000 [==============================] - 3s 22us/sample - loss: 0.3520 - binary_accuracy: 0.8418\n",
      "Epoch 5/10\n",
      "150000/150000 [==============================] - 3s 22us/sample - loss: 0.3399 - binary_accuracy: 0.8486\n",
      "Epoch 6/10\n",
      "150000/150000 [==============================] - 3s 21us/sample - loss: 0.3274 - binary_accuracy: 0.8550\n",
      "Epoch 7/10\n",
      "150000/150000 [==============================] - 3s 19us/sample - loss: 0.3151 - binary_accuracy: 0.8621\n",
      "Epoch 8/10\n",
      "150000/150000 [==============================] - 3s 20us/sample - loss: 0.3024 - binary_accuracy: 0.8682\n",
      "Epoch 9/10\n",
      "150000/150000 [==============================] - 3s 22us/sample - loss: 0.2908 - binary_accuracy: 0.8744\n",
      "Epoch 10/10\n",
      "150000/150000 [==============================] - 3s 22us/sample - loss: 0.2792 - binary_accuracy: 0.8798\n",
      "50000/50000 [==============================] - 2s 41us/sample - loss: 0.4000 - binary_accuracy: 0.8268\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(1000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss=losses.binary_crossentropy,\n",
    "             metrics=[metrics.binary_accuracy])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['마이셀럽스는 어떤 회사?마이셀럽스는국내 최초로 인공지능을 활용한 빅데이터 기반의 취향 검색 서비스를 제공하는 회사 이다.년 월맥킨지컴퍼니 디지털전략 부문을 담당했고 그룹 최고디지털책임자부사장를 지낸 도준웅 씨가 설립했다. 자체 개발한 인공지능 솔루션 시스템인를 운영하며 데이터의 수집',\n",
       " '시각화',\n",
       " '지능탑재',\n",
       " '라이브 업데이트 등의 서비스를 제공한다. 이 시스템은 아마존웹서비스로부터즉시 수익화가 가능한 인공지능 솔루션 이라는 평가를 받았다. 부킹닷컴',\n",
       " '신세계면세점']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "feedback_list=[]\n",
    "\n",
    "with open('../../(1) cleaner/mycelebs_content.csv', 'r', encoding='utf-8') as f:\n",
    "    csv_reader = csv.reader(f, delimiter = '\\n')\n",
    "    for row in csv_reader:\n",
    "        feedback_list.append(row)\n",
    "\n",
    "feedback_list = sum(feedback_list, [])\n",
    "feedback_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1393"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feedback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos_neg_fix(review):\n",
    "    token = tokenize(review)\n",
    "    tf = term_frequency(token)\n",
    "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data))\n",
    "    if(score > 0.5):\n",
    "        positive001.append(\"[{}], 긍정 텍스트 확률 : {:.2f}%\".format(review, score * 100))\n",
    "    else:\n",
    "        negative001.append(\"[{}], 부정 텍스트 확률 : {:.2f}%\".format(review, (1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive001 = []\n",
    "negative001 = []\n",
    "\n",
    "for i in range(0, len(feedback_list)):\n",
    "    predict_pos_neg_fix(str(feedback_list[i]))\n",
    "\n",
    "import pandas as pd\n",
    "evaluate_pos001 = pd.DataFrame(positive001)\n",
    "evaluate_pos001.to_csv(\"./mycelebs_content_positive.csv\", header = False, index = False)\n",
    "evaluate_neg001 = pd.DataFrame(negative001)\n",
    "evaluate_neg001.to_csv(\"./mycelebs_content_negative.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
