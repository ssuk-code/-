{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분유 제조사 목록\n",
    "milkName = ['갈리아', '거버', '남양유업', '네슬레', '네이쳐스원', '노발락', '뉴트리시아', '뢰벤짠', '매일유업', '메이지', '밀라산', '밀루파', \n",
    "            '벨라미스오가닉', '아스펜', '아이배냇', '아이엠뉴질랜드', '애보트', '얼스베스트', '엔파밀', '오브맘', '와코도', '일동후디스', '카브리타',\n",
    "            '커클랜드', '퇴퍼', '파스퇴르', '페디아슈어', '퓨어랜드', '후마나', 'HOLLE', 'Hipp', 'LG생활건강']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowfood = ['남양유업',  '네슬레', '래퍼티스가든', '맘스쌀과자', '매일유업', '미즈앤코', '베베쿡', '비치넛', '에바토', '일동후디스', '파스퇴르', \n",
    "            'LG생활건강', 'SafrunatS.L', '루솔', '뽀뽀뜨', '아이배냇', '얼스베스트', '엘빈즈', '오가닉맘', '짱죽', '팜투베이비', '푸드케어', '풀무원']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gijugi = [ '겐키', '군', '깨끗한나라', '나비잠', '네띠', '네이쳐러브메레', '네추럴블라썸', '네츄라오가닉', '대디베이비', '드라이퍼스', '로맘스', \n",
    "          '로한', '리베로', '리보쓰', '리에또', '마르마이플러스', '마미포코', '맘스네이처', '메리즈', '메리트산업', '메이커키즈', '모나리자', \n",
    "          '모모래빗', '모이몰른', '몰텍스', '무니', '무루', '무미', '밍크뮤', '바바랩', '밤보', '밤부베베', '백양산업', '범킨스', '베베린도',\n",
    "           '베베몬', '베베원', '베이비리', '베이비스토리', '베이비앙', '베이비앤아이', '베이비오가닉', '베피스', '벤스랜드', '보솜이', \n",
    "          '블루독베이비', '서림', '세븐스제너레이션', '세한', '쇼콜라', '수안산업', '슈퍼대디', '신성', '쌍용C&B', '아가똥', '아가짱', '아이두레',\n",
    "           '아이씨프로젝트', '아이에이커머스', '아이엠트루', '아이웰', '아이티씨', '암웨이', '압소바', '앙블랑', '애플크럼비', '에뜨와', \n",
    "          '에이메르네이쳐', '에코튜드네이처', '에프랑', '엠케이', '영림B&A', '오가닉맘', '오로라', '오보소', '오아이비', '올러브', '유트러스', \n",
    "          '은홍섬유', '이마트', '이엔컴퍼니', '이오스트', '일동아이엠씨', '조이로이', '주치니', '주타노다이퍼', '참사랑', '천싸요', '첨이첨이', \n",
    "          '치쿨', '캉가케어', '커클랜드', '케이맘', '코트니', '코튼리퍼블릭', '쿠쉬스', '쿠잉', '큐비앤맘', '클로즈', '키즈디조이', '킨도', '킷앤킨',\n",
    "           '토디앙', '파라솔', '파코라반베이비', '팸퍼스', '페넬로페', '펠트친구', '포그니', '포유', '폴라리스', '폴맘', '피노', '핑고', '하기스', \n",
    "          '하이가', '한빛산업', '행키베베', '헤인셀레스티얼', '현대BABY산업', '홈플러스', 'BB DISTRIBE', 'GBSTYLE', 'GERBER', 'MK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyCar = [ '구름과환경', '구비', '굿베이비', '그라코', '그린탐', '깜', '나단스', '뉴나', '다이치', '델타', '도렐', '동방레포츠', '라스깔라', '레카로', '로드스타', '리안', \n",
    "           '리첼', '리틀타익스', '린덴', '마루스', '마리꼬베베', '마마루', '마마스앤파파스', '마이크라라이트', '맥시코시', '맥클라렌', '몽트뢰', '무무브', '무치', '미마', \n",
    "           '바론', '밤비노', '베이비몬스터', '베이비스타일', '베이비시티', '베이비싱', '베이비젠', '베이비조거', '베이비캠프', '베이비트랜드', '베이비프라임', '베이비홈', \n",
    "           '보브코리아', '부가부', '브라이택스', '브레비', '뻬그뻬레고', '샤오미', '세기산업', '세이프티퍼스트', '세피앙', '스퀴즈', '스토케', '스트롤에어', '실버크로스', \n",
    "           '실버팍스', '싸이벡스', '써니러브', '아가방앤컴퍼니', '아넥스', '아발론', '아이빌리브', '아이사랑', '아이엠베이비', '아이캔디', '아이쿠', '아프리카', '알로앤루', \n",
    "           '알퐁소', '앙팡스', '어파베이비', '에그', '에르고베이비', '에스디스피드', '에어보스', '에이블트레이딩', '엔픽스', '엘레갈로', '엘레니어', '엠버', '예떼', '오르빗베이비', \n",
    "           '오타브', '와이업', '와이케이비앤씨', '요야', '유니원', '이바겐', '이븐플로', '이지워커', '인스텝', '잉글라차','잉글레시나', '제로', '제인', '조이', '조이파트너스', \n",
    "           '조코', '줄즈', '중모토이플러스', '쥬비', '지비', '쭈쭈베베', '차일드홈', '치코', '카펠라', '코뮌코리아', '코사토', '코엔코', '콜크래프트', '콤비', '콩코드', '퀴니', \n",
    "           '큐터스', '키도포타머스', '키디', '키즈엠브레이스', '킨더웨건', '킨즈', '테크넘', '토이파파', '톨스토이', '톰비', '툴레', '파파앤코', '팔리', '페도라', '포브', '프시케', \n",
    "           '필앤테즈', '하이브리드', '한스', '한일레인보우', '해님', '해밀턴', '해피랜드F&C', '햇님토이', '헨티', 'ABC', 'GB', 'MEIBAO', 'SEEC', 'VOVO', 'belecoo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "milkName_P = []\n",
    "milkName_N = []\n",
    "milkName_P_len = []\n",
    "milkName_N_len = []\n",
    "\n",
    "for milk in milkName:\n",
    "    pos001_001 = pd.read_csv(f\"./1. 분유/{milk}_positive.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    pos001_001['text'] = pos001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    pos001_001['pos_test'] = pos001_001.text.str.split('긍정 텍스트 확률').str[1]\n",
    "    pos001_002 = pos001_001\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace(', 긍정 텍스트 확률 :', '')\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    pos001_002['pos_test'] = pos001_002['pos_test'].str.replace('긍정 텍스트 확률 : ', '')\n",
    "    pos001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    pos001_003 = pos001_002\n",
    "    pos001_003.dropna(subset = ['text'], inplace=True)\n",
    "    pos_test = pos001_003.reset_index(drop=True)\n",
    "\n",
    "    neg001_001 = pd.read_csv(f\"./1. 분유/{milk}_negative.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    neg001_001['text'] = neg001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    neg001_001['neg_test'] = neg001_001.text.str.split('부정 텍스트 확률').str[1]\n",
    "    neg001_002 = neg001_001\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace(', 부정 텍스트 확률 :', '')\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    neg001_002['neg_test'] = neg001_002['neg_test'].str.replace('부정 텍스트 확률 : ', '')\n",
    "    neg001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    neg001_003 = neg001_002\n",
    "    neg001_003.dropna(subset = ['text'], inplace=True)\n",
    "    neg_test = neg001_003.reset_index(drop=True)\n",
    "\n",
    "    pos_total001 = pos_test.iloc[:, [1]]\n",
    "    neg_total001 = neg_test.iloc[:, [1]]\n",
    "\n",
    "    pos_total001['emotion'] = '긍정'\n",
    "    pos_total001.rename(columns = {'pos_test' : 'test_degree'}, inplace=True)\n",
    "    pos_total001['test_degree'] = pos_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    pos_total001['test_degree'] = pd.to_numeric(pos_total001['test_degree'])\n",
    "    neg_total001['emotion'] = '부정'\n",
    "    neg_total001.rename(columns = {'neg_test' : 'test_degree'}, inplace=True)\n",
    "    neg_total001['test_degree'] = neg_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    neg_total001['test_degree'] = pd.to_numeric(neg_total001['test_degree'])\n",
    "\n",
    "    total_mean = pd.DataFrame({'감정 정도 평균값 (%)' : [pos_total001['test_degree'].mean(), neg_total001['test_degree'].mean()], '감정 분류' : ['긍정', '부정']})\n",
    "    total_mean.to_csv(f\"./5. 감정 평균/1. 분유/{milk}_emotion.csv\", header = False, index = False)\n",
    "\n",
    "    milkName_P.append(pos_total001['test_degree'].mean())\n",
    "    milkName_N.append(neg_total001['test_degree'].mean())\n",
    "    milkName_P_len.append(len(pos001_001))\n",
    "    milkName_N_len.append(len(neg001_001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowfood_P = []\n",
    "slowfood_N = []\n",
    "slowfood_P_len = []\n",
    "slowfood_N_len = []\n",
    "\n",
    "for milk in slowfood:\n",
    "    pos001_001 = pd.read_csv(f\"./2. 이유식/{milk}_positive.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    pos001_001['text'] = pos001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    pos001_001['pos_test'] = pos001_001.text.str.split('긍정 텍스트 확률').str[1]\n",
    "    pos001_002 = pos001_001\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace(', 긍정 텍스트 확률 :', '')\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    pos001_002['pos_test'] = pos001_002['pos_test'].str.replace('긍정 텍스트 확률 : ', '')\n",
    "    pos001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    pos001_003 = pos001_002\n",
    "    pos001_003.dropna(subset = ['text'], inplace=True)\n",
    "    pos_test = pos001_003.reset_index(drop=True)\n",
    "\n",
    "    neg001_001 = pd.read_csv(f\"./2. 이유식/{milk}_negative.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    neg001_001['text'] = neg001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    neg001_001['neg_test'] = neg001_001.text.str.split('부정 텍스트 확률').str[1]\n",
    "    neg001_002 = neg001_001\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace(', 부정 텍스트 확률 :', '')\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    neg001_002['neg_test'] = neg001_002['neg_test'].str.replace('부정 텍스트 확률 : ', '')\n",
    "    neg001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    neg001_003 = neg001_002\n",
    "    neg001_003.dropna(subset = ['text'], inplace=True)\n",
    "    neg_test = neg001_003.reset_index(drop=True)\n",
    "\n",
    "    pos_total001 = pos_test.iloc[:, [1]]\n",
    "    neg_total001 = neg_test.iloc[:, [1]]\n",
    "\n",
    "    pos_total001['emotion'] = '긍정'\n",
    "    pos_total001.rename(columns = {'pos_test' : 'test_degree'}, inplace=True)\n",
    "    pos_total001['test_degree'] = pos_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    pos_total001['test_degree'] = pd.to_numeric(pos_total001['test_degree'])\n",
    "    neg_total001['emotion'] = '부정'\n",
    "    neg_total001.rename(columns = {'neg_test' : 'test_degree'}, inplace=True)\n",
    "    neg_total001['test_degree'] = neg_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    neg_total001['test_degree'] = pd.to_numeric(neg_total001['test_degree'])\n",
    "\n",
    "    total_mean = pd.DataFrame({'감정 정도 평균값 (%)' : [pos_total001['test_degree'].mean(), neg_total001['test_degree'].mean()], '감정 분류' : ['긍정', '부정']})\n",
    "    total_mean.to_csv(f\"./5. 감정 평균/2. 이유식/{milk}_emotion.csv\", header = False, index = False)    \n",
    "    \n",
    "    slowfood_P.append(pos_total001['test_degree'].mean())\n",
    "    slowfood_N.append(neg_total001['test_degree'].mean())\n",
    "    slowfood_P_len.append(len(pos001_001))\n",
    "    slowfood_N_len.append(len(neg001_001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gijugi_P = []\n",
    "gijugi_N = []\n",
    "gijugi_P_len = []\n",
    "gijugi_N_len = []\n",
    "\n",
    "for milk in gijugi:\n",
    "    pos001_001 = pd.read_csv(f\"./3. 기저귀/{milk}_positive.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    pos001_001['text'] = pos001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    pos001_001['pos_test'] = pos001_001.text.str.split('긍정 텍스트 확률').str[1]\n",
    "    pos001_002 = pos001_001\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace(', 긍정 텍스트 확률 :', '')\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    pos001_002['pos_test'] = pos001_002['pos_test'].str.replace('긍정 텍스트 확률 : ', '')\n",
    "    pos001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    pos001_003 = pos001_002\n",
    "    pos001_003.dropna(subset = ['text'], inplace=True)\n",
    "    pos_test = pos001_003.reset_index(drop=True)\n",
    "\n",
    "    neg001_001 = pd.read_csv(f\"./3. 기저귀/{milk}_negative.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    neg001_001['text'] = neg001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    neg001_001['neg_test'] = neg001_001.text.str.split('부정 텍스트 확률').str[1]\n",
    "    neg001_002 = neg001_001\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace(', 부정 텍스트 확률 :', '')\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    neg001_002['neg_test'] = neg001_002['neg_test'].str.replace('부정 텍스트 확률 : ', '')\n",
    "    neg001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    neg001_003 = neg001_002\n",
    "    neg001_003.dropna(subset = ['text'], inplace=True)\n",
    "    neg_test = neg001_003.reset_index(drop=True)\n",
    "\n",
    "    pos_total001 = pos_test.iloc[:, [1]]\n",
    "    neg_total001 = neg_test.iloc[:, [1]]\n",
    "\n",
    "    pos_total001['emotion'] = '긍정'\n",
    "    pos_total001.rename(columns = {'pos_test' : 'test_degree'}, inplace=True)\n",
    "    pos_total001['test_degree'] = pos_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    pos_total001['test_degree'] = pd.to_numeric(pos_total001['test_degree'])\n",
    "    neg_total001['emotion'] = '부정'\n",
    "    neg_total001.rename(columns = {'neg_test' : 'test_degree'}, inplace=True)\n",
    "    neg_total001['test_degree'] = neg_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    neg_total001['test_degree'] = pd.to_numeric(neg_total001['test_degree'])\n",
    "\n",
    "    total_mean = pd.DataFrame({'감정 정도 평균값 (%)' : [pos_total001['test_degree'].mean(), neg_total001['test_degree'].mean()], '감정 분류' : ['긍정', '부정']})\n",
    "    total_mean.to_csv(f\"./5. 감정 평균/3. 기저귀/{milk}_emotion.csv\", header = False, index = False)\n",
    "        \n",
    "    gijugi_P.append(pos_total001['test_degree'].mean())\n",
    "    gijugi_N.append(neg_total001['test_degree'].mean())\n",
    "    gijugi_P_len.append(len(pos001_001))\n",
    "    gijugi_N_len.append(len(neg001_001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyCar_P = []\n",
    "babyCar_N = []\n",
    "babyCar_P_len = []\n",
    "babyCar_N_len = []\n",
    "\n",
    "for milk in babyCar:\n",
    "    pos001_001 = pd.read_csv(f\"./4. 유모차/{milk}_positive.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    pos001_001['text'] = pos001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    pos001_001['pos_test'] = pos001_001.text.str.split('긍정 텍스트 확률').str[1]\n",
    "    pos001_002 = pos001_001\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace(', 긍정 텍스트 확률 :', '')\n",
    "    pos001_002['text'] = pos001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    pos001_002['pos_test'] = pos001_002['pos_test'].str.replace('긍정 텍스트 확률 : ', '')\n",
    "    pos001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    pos001_003 = pos001_002\n",
    "    pos001_003.dropna(subset = ['text'], inplace=True)\n",
    "    pos_test = pos001_003.reset_index(drop=True)\n",
    "\n",
    "    neg001_001 = pd.read_csv(f\"./4. 유모차/{milk}_negative.csv\", encoding = 'utf-8', names = ['text'])\n",
    "    neg001_001['text'] = neg001_001['text'].str.replace('[-=+#/\\?^$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\…》]', '')\n",
    "    neg001_001['neg_test'] = neg001_001.text.str.split('부정 텍스트 확률').str[1]\n",
    "    neg001_002 = neg001_001\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace(', 부정 텍스트 확률 :', '')\n",
    "    neg001_002['text'] = neg001_002['text'].str.replace('[.-=+#/\\?^%$@*\\\"※~&ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '')\n",
    "    neg001_002['neg_test'] = neg001_002['neg_test'].str.replace('부정 텍스트 확률 : ', '')\n",
    "    neg001_002['text'].replace('  ', np.nan, inplace=True)\n",
    "    neg001_003 = neg001_002\n",
    "    neg001_003.dropna(subset = ['text'], inplace=True)\n",
    "    neg_test = neg001_003.reset_index(drop=True)\n",
    "\n",
    "    pos_total001 = pos_test.iloc[:, [1]]\n",
    "    neg_total001 = neg_test.iloc[:, [1]]\n",
    "\n",
    "    pos_total001['emotion'] = '긍정'\n",
    "    pos_total001.rename(columns = {'pos_test' : 'test_degree'}, inplace=True)\n",
    "    pos_total001['test_degree'] = pos_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    pos_total001['test_degree'] = pd.to_numeric(pos_total001['test_degree'])\n",
    "    neg_total001['emotion'] = '부정'\n",
    "    neg_total001.rename(columns = {'neg_test' : 'test_degree'}, inplace=True)\n",
    "    neg_total001['test_degree'] = neg_total001['test_degree'].str.replace('[:% ]', '')\n",
    "    neg_total001['test_degree'] = pd.to_numeric(neg_total001['test_degree'])\n",
    "\n",
    "    total_mean = pd.DataFrame({'감정 정도 평균값 (%)' : [pos_total001['test_degree'].mean(), neg_total001['test_degree'].mean()], '감정 분류' : ['긍정', '부정']})\n",
    "    total_mean.to_csv(f\"./5. 감정 평균/4. 유모차/{milk}_emotion.csv\", header = False, index = False)\n",
    "        \n",
    "    babyCar_P.append(pos_total001['test_degree'].mean())\n",
    "    babyCar_N.append(neg_total001['test_degree'].mean())\n",
    "    babyCar_P_len.append(len(pos001_001))\n",
    "    babyCar_N_len.append(len(neg001_001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "like = []\n",
    "for i in range(0, len(milkName)):\n",
    "    like.append(((milkName_P_len[i]*milkName_P[i])/((milkName_P_len[i]*milkName_P[i])+(milkName_N[i]*milkName_N_len[i])))*100)\n",
    "df = pd.DataFrame(data={'제조사':milkName, 'like':like})\n",
    "df = df.sort_values(by=['like'], axis=0, ascending=False)\n",
    "df = df.dropna()\n",
    "df.to_csv(f\"./분유 제조사별 선호도.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "like = []\n",
    "for i in range(0, len(slowfood)):\n",
    "    like.append(((slowfood_P_len[i]*slowfood_P[i])/((slowfood_P_len[i]*slowfood_P[i])+(slowfood_N[i]*slowfood_N_len[i])))*100)\n",
    "df = pd.DataFrame(data={'제조사':slowfood, 'like':like})\n",
    "df = df.sort_values(by=['like'], axis=0, ascending=False)\n",
    "df = df.dropna()\n",
    "df.to_csv(f\"./이유식 제조사별 선호도.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "like = []\n",
    "for i in range(0, len(gijugi)):\n",
    "    like.append(((gijugi_P_len[i]*gijugi_P[i])/((gijugi_P_len[i]*gijugi_P[i])+(gijugi_N[i]*gijugi_N_len[i])))*100)\n",
    "df = pd.DataFrame(data={'제조사':gijugi, 'like':like})\n",
    "df = df.sort_values(by=['like'], axis=0, ascending=False)\n",
    "df = df.dropna()\n",
    "df.to_csv(f\"./기저귀 제조사별 선호도.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "like = []\n",
    "for i in range(0, len(babyCar)):\n",
    "    like.append(((babyCar_P_len[i]*babyCar_P[i])/((babyCar_P_len[i]*babyCar_P[i])+(babyCar_N[i]*babyCar_N_len[i])))*100)\n",
    "df = pd.DataFrame(data={'제조사':babyCar, 'like':like})\n",
    "df = df.sort_values(by=['like'], axis=0, ascending=False)\n",
    "df = df.dropna()\n",
    "df.to_csv(f\"./유모차 제조사별 선호도.csv\", header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
